#!/usr/bin/env python
from __future__ import annotations
import argparse, json, sys
from pathlib import Path
from datetime import datetime
import joblib
import pandas as pd
import numpy as np

from algolab_ml.pipelines import tabular as tabular_pipeline
from algolab_ml.models import model_zoo
from algolab_ml.data.cleaning import (
    basic_clean, fill_na, enforce_schema, drop_constant_cols,
    clip_outliers, bucket_rare_categories, parse_dates
)
from algolab_ml.utils.config import load_json_or_yaml
from algolab_ml.features.builder import FeatureBuilder

# ----------------- å·¥å…·å‡½æ•° -----------------
def _sec(title: str):
    print(f"\n------- {title} -------")

def _print_df_shape(df: pd.DataFrame, note: str):
    print(f"{note}ï¼š{df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")

def _parse_params(s: str | None) -> dict:
    if not s: return {}
    s = s.strip()
    if s.startswith("@"):
        p = Path(s[1:])
        data = json.loads(p.read_text(encoding="utf-8"))
        if not isinstance(data, dict):
            raise ValueError("params file must beä¸€ä¸ª JSON å¯¹è±¡")
        return data
    try:
        data = json.loads(s)
        if isinstance(data, dict): return data
    except Exception:
        pass
    out = {}
    for part in s.split(","):
        part = part.strip()
        if not part: continue
        if "=" not in part: raise ValueError(f"Bad param '{part}', expected key=value")
        k, v = part.split("=", 1); k, v = k.strip(), v.strip()
        if v.lower() in ("true","false"): out[k] = (v.lower()=="true"); continue
        try: out[k] = int(v); continue
        except: pass
        try: out[k] = float(v); continue
        except: pass
        out[k] = v
    return out

def _print_list_models(fmt: str, task: str):
    data = {
        "classification": {
            "canonical": model_zoo.available_models("classification"),
            "aliases": model_zoo.aliases_for_task("classification"),
        },
        "regression": {
            "canonical": model_zoo.available_models("regression"),
            "aliases": model_zoo.aliases_for_task("regression"),
        },
    }
    if fmt == "json":
        print(json.dumps(data, ensure_ascii=False, indent=2))
    else:
        scopes = ["classification","regression"] if task=="all" else [task]
        for sc in scopes:
            print(f"[{sc}]")
            print("  models: " + ", ".join(sorted(data[sc]["canonical"])))
            print("  aliases:")
            for k, v in sorted(data[sc]["aliases"].items()):
                print(f"    {k} -> {v}")

def _print_params(name: str):
    out = {}
    for task in ("classification","regression"):
        try:
            out[task] = model_zoo.model_signature(name, task)
        except KeyError:
            pass
    if not out:
        avail = {
            "classification": model_zoo.available_models("classification"),
            "regression": model_zoo.available_models("regression"),
        }
        raise SystemExit(f"Unknown model alias '{name}'. Available: {json.dumps(avail, ensure_ascii=False)}")
    print(json.dumps(out, ensure_ascii=False, indent=2))

# ----------------- å¸¦æ—¥å¿—çš„æ¸…æ´—æ‰§è¡Œ -----------------
def _apply_cleaning_with_log(df: pd.DataFrame, cfg: dict, target: str | None = None) -> pd.DataFrame:
    cur = df.copy()
    _sec("å¼€å§‹æ•°æ®æ¸…æ´—ï¼ˆé…ç½®å¼ï¼‰")
    _print_df_shape(cur, "åŸå§‹æ•°æ®ç»´åº¦")

    # 1) enforce_schema
    es = cfg.get("enforce_schema")
    if es:
        _sec("Schema è§„èŒƒåŒ–")
        rename = es.get("rename") or {}
        required = es.get("required") or []
        dtypes = es.get("dtypes") or {}
        if rename:
            print(f"é‡å‘½ååˆ—ï¼š{rename}")
        if required:
            print(f"å¿…éœ€åˆ—ï¼š{required}")
        if dtypes:
            print(f"ç±»å‹çº¦æŸï¼ˆç›®æ ‡ç±»å‹ï¼‰ï¼š{dtypes}")
        cur = enforce_schema(cur, dtypes=dtypes, rename=rename, required=required)
        _print_df_shape(cur, "è§„èŒƒåŒ–åç»´åº¦")

    # 2) basic_clean
    bc = cfg.get("basic_clean")
    if bc:
        drop_cols = bc.get("drop_cols") if isinstance(bc, dict) else None
        dup_cnt = cur.duplicated().sum()
        before_cols = set(cur.columns)
        cur2 = basic_clean(cur, drop_cols=drop_cols)
        dropped_cols = list(before_cols - set(cur2.columns))
        print(f"å»é‡è¡Œæ•°ï¼š{dup_cnt}")
        if drop_cols:
            kept = [c for c in drop_cols if c in before_cols]
            print(f"åˆ é™¤æŒ‡å®šåˆ—ï¼š{kept or []}")
        if dropped_cols:
            print(f"ï¼ˆå«é‡å¤å/æ— æ•ˆå¯¼è‡´çš„ï¼‰å®é™…è¢«åˆ é™¤åˆ—ï¼š{dropped_cols}")
        cur = cur2
        _print_df_shape(cur, "basic_clean åç»´åº¦")

    # 3) fill_na
    if cfg.get("fill_na"):
        na = cfg["fill_na"]
        num = na.get("num","median")
        cat = na.get("cat","most_frequent")
        num_cols = cur.select_dtypes(include=["number"]).columns
        cat_cols = [c for c in cur.columns if c not in num_cols]
        n_na_num_before = int(cur[num_cols].isna().sum().sum()) if len(num_cols)>0 else 0
        n_na_cat_before = int(cur[cat_cols].isna().sum().sum()) if len(cat_cols)>0 else 0
        cur2 = fill_na(cur, num_strategy=num, cat_strategy=cat)
        n_na_num_after = int(cur2[num_cols].isna().sum().sum()) if len(num_cols)>0 else 0
        n_na_cat_after = int(cur2[cat_cols].isna().sum().sum()) if len(cat_cols)>0 else 0
        _sec("ç¼ºå¤±å€¼å¡«å……")
        print(f"æ•°å€¼åˆ—å¡«å……ç­–ç•¥ï¼š{num}ï¼Œç¼ºå¤± {n_na_num_before} -> {n_na_num_after}")
        print(f"ç±»åˆ«åˆ—å¡«å……ç­–ç•¥ï¼š{cat}ï¼Œç¼ºå¤± {n_na_cat_before} -> {n_na_cat_after}")
        cur = cur2

    # 4) drop_constant
    if cfg.get("drop_constant"):
        th = cfg["drop_constant"].get("threshold_unique", 1)
        const_cols = [c for c in cur.columns if cur[c].nunique(dropna=True) <= th]
        cur = drop_constant_cols(cur, threshold_unique=th)
        _sec("åˆ é™¤å¸¸é‡åˆ—")
        print(f"é˜ˆå€¼ï¼šå”¯ä¸€å€¼ â‰¤ {th}ï¼Œåˆ é™¤åˆ—ï¼š{const_cols}")

    # 5) clip_outliers
    if cfg.get("clip_outliers"):
        co = cfg["clip_outliers"]
        method = co.get("method","iqr")
        zt = co.get("z_thresh",3.0)
        ik = co.get("iqr_k",1.5)
        cols = co.get("cols")
        # è‡ªåŠ¨é€‰æ‹©æ•°å€¼åˆ—
        if cols is None:
            cols = cur.select_dtypes(include=["number"]).columns.tolist()
        # æ’é™¤ç›®æ ‡åˆ—ï¼ˆé¿å…å¯¹ label/ç›®æ ‡åšæˆªæ–­ï¼‰
        if target and target in cols:
            cols = [c for c in cols if c != target]
            print(f"å·²è‡ªåŠ¨æ’é™¤ç›®æ ‡åˆ—ï¼š{target}")

        # å¦‚æœå‰©ä½™å¯å¤„ç†åˆ—ä¸ºç©ºï¼Œè·³è¿‡
        if not cols:
            _sec("å¼‚å¸¸å€¼æˆªæ–­")
            print("æ²¡æœ‰å¯å¤„ç†çš„æ•°å€¼åˆ—ï¼Œè·³è¿‡ã€‚")
        else:
            before = cur[cols].copy()
            cur = clip_outliers(cur, cols=cols, method=method, z_thresh=zt, iqr_k=ik)
            changed = {c: int((cur[c] != before[c]).sum()) for c in cols if c in cur.columns}
            total_changed = int(sum(changed.values()))
            _sec("å¼‚å¸¸å€¼æˆªæ–­")
            print(f"æ–¹å¼ï¼š{method}ï¼ˆz={zt} / iqr_k={ik}ï¼‰ï¼Œå¤„ç†åˆ—æ•°ï¼š{len(cols)}ï¼Œè¢«æˆªæ–­çš„å•å…ƒæ ¼æ€»æ•°ï¼š{total_changed}")
            if total_changed:
                top = sorted(changed.items(), key=lambda x: x[1], reverse=True)[:8]
                print(f"å˜åŒ–æœ€å¤šçš„åˆ—ï¼ˆå‰ 8ï¼‰ï¼š{top}")
    # 6) bucket_rare
    if cfg.get("bucket_rare"):
        br = cfg["bucket_rare"]
        cols = br.get("cols", [])
        minf = br.get("min_freq", 10)
        other = br.get("other_label","_OTHER")
        counts = {}
        for c in cols:
            if c in cur.columns:
                vc = cur[c].astype(str).value_counts(dropna=False)
                counts[c] = int((vc < minf).sum())
        cur = bucket_rare_categories(cur, cols=cols, min_freq=minf, other_label=other)
        _sec("ä½é¢‘ç±»åˆ«åˆå¹¶")
        print(f"åˆ—ï¼š{cols}ï¼Œé¢‘æ¬¡é˜ˆå€¼ï¼š{minf}ï¼Œåˆå¹¶æ ‡ç­¾ï¼š{other}")
        if counts:
            print(f"å„åˆ—ä½é¢‘ç±»åˆ«ä¸ªæ•°ï¼š{counts}")

    # 7) parse_dates
    if cfg.get("parse_dates"):
        mapping = cfg["parse_dates"]
        cur = parse_dates(cur, mapping)
        _sec("æ—¥æœŸè§£æ")
        info = {}
        for col in mapping:
            if col in cur.columns:
                ok = int(cur[col].notna().sum())
                na = int(cur[col].isna().sum())
                info[col] = {"å¯è§£æ": ok, "NaT": na}
        print(f"è§£æåˆ—ï¼š{mapping}ï¼Œç»“æœï¼š{info}")

    _sec("æ¸…æ´—å®Œæˆ")
    _print_df_shape(cur, "æœ€ç»ˆæ•°æ®ç»´åº¦")
    return cur

# ----------------- ä¸»ç¨‹åº -----------------
def main():
    ap = argparse.ArgumentParser(description="Algolab ML Runnerï¼ˆå¸¦æ¸…æ´—æ—¥å¿—ï¼‰")
    ap.add_argument("--csv", help="è®­ç»ƒæ•°æ® CSV è·¯å¾„")
    ap.add_argument("--target", help="ç›®æ ‡åˆ—å")
    ap.add_argument("--model", default="xgb", help="xgb|lgbm|catboost|rf|gbdt|logreg|ridge|lassoï¼ˆæ”¯æŒåˆ«åï¼‰")
    ap.add_argument("--test_size", type=float, default=0.2)
    ap.add_argument("--random_state", type=int, default=42)
    ap.add_argument("--no-preprocess", dest="no_preprocess", action="store_true", help="å…³é—­æ ‡å‡†åŒ–/One-Hot ç­‰é¢„å¤„ç†")
    ap.add_argument("--export", action="store_true", help="ä¿å­˜æ¨¡å‹ä¸æŠ¥å‘Šåˆ° --out-dirï¼ˆæœªæŒ‡å®šåˆ™ runs/æ—¶é—´æˆ³ï¼‰")
    ap.add_argument("--out-dir", default=None, help="ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ runs/YYYYmmdd-HHMMSSï¼‰")
    ap.add_argument("--params", default=None, help="è¶…å‚ï¼šJSON æˆ– 'k=v,k2=v2'ï¼›ä¹Ÿæ”¯æŒ @/path/params.json")
    # æ¸…æ´—
    ap.add_argument("--clean-config", default=None, help="æ¸…æ´—é…ç½®ï¼šJSON æ–‡æœ¬æˆ– @/path/to/clean.yml(JSON/YAML)")
    ap.add_argument("--clean-basic", action="store_true", help="æ‰§è¡Œ basic_cleanï¼ˆå»é‡ã€åˆ—åæ¸…ç†ç­‰ï¼‰")
    ap.add_argument("--fill-na", default=None, help="ç¼ºå¤±å€¼ç­–ç•¥ï¼š'num=median,cat=most_frequent' æˆ– JSON")
    ap.add_argument("--feat-config", default=None, help="ç‰¹å¾å·¥ç¨‹é…ç½®ï¼šJSON æ–‡æœ¬æˆ– @/path/to/features.yml(JSON/YAML)")
    # å‘ç°æ€§
    ap.add_argument("--list-models", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹ä¸åˆ«å")
    ap.add_argument("--format", choices=["json","text"], default="json", help="list è¾“å‡ºæ ¼å¼")
    ap.add_argument("--task", choices=["classification","regression","all"], default="all", help="list ä½œç”¨åŸŸ")
    ap.add_argument("--show-params", default=None, help="æ˜¾ç¤ºæŸæ¨¡å‹æ„é€ å‡½æ•°å‚æ•°ç­¾ååé€€å‡º")
    args, unknown = ap.parse_known_args()

    # å‘ç°å‹å‘½ä»¤
    if args.list_models:
        _print_list_models(args.format, args.task); return
    if args.show_params:
        _print_params(args.show_params); return

    if not args.csv or not args.target:
        raise SystemExit("ç¼ºå°‘ --csv / --targetã€‚ä¹Ÿå¯ä»¥å…ˆç”¨ --list-models æˆ– --show-paramsã€‚")

    # è¯»å– & æ¸…æ´—
    df = pd.read_csv(args.csv)
    cfg = load_json_or_yaml(args.clean_config)

    if cfg:
        df = _apply_cleaning_with_log(df, cfg, target=args.target)
    else:
        if args.clean_basic:
            _sec("basic_clean")
            _print_df_shape(df, "æ¸…æ´—å‰")
            dup_cnt = df.duplicated().sum()
            df = basic_clean(df)
            print(f"å»é‡è¡Œæ•°ï¼š{dup_cnt}")
            _print_df_shape(df, "æ¸…æ´—å")
        if args.fill_na:
            _sec("ç¼ºå¤±å€¼å¡«å……")
            try:
                if args.fill_na.strip().startswith("{"):
                    na = json.loads(args.fill_na)
                    num = na.get("num","median"); cat = na.get("cat","most_frequent")
                else:
                    parts = dict(p.split("=") for p in args.fill_na.split(","))
                    num = parts.get("num","median"); cat = parts.get("cat","most_frequent")
            except Exception:
                num, cat = "median", "most_frequent"
            num_cols = df.select_dtypes(include=["number"]).columns
            cat_cols = [c for c in df.columns if c not in num_cols]
            n_na_num_before = int(df[num_cols].isna().sum().sum()) if len(num_cols)>0 else 0
            n_na_cat_before = int(df[cat_cols].isna().sum().sum()) if len(cat_cols)>0 else 0
            df = fill_na(df, num_strategy=num, cat_strategy=cat)
            n_na_num_after = int(df[num_cols].isna().sum().sum()) if len(num_cols)>0 else 0
            n_na_cat_after = int(df[cat_cols].isna().sum().sum()) if len(cat_cols)>0 else 0
            print(f"æ•°å€¼ç¼ºå¤±ï¼š{n_na_num_before} -> {n_na_cat_after}")
            print(f"ç±»åˆ«ç¼ºå¤±ï¼š{n_na_cat_before} -> {n_na_cat_after}")
            _print_df_shape(df, "å¡«å……å")
    
    # æ¸…æ´—å®Œæˆå
    feat_cfg = load_json_or_yaml(args.feat_config)
    if feat_cfg:
        print("\n------- å¼€å§‹ç‰¹å¾å·¥ç¨‹ï¼ˆé…ç½®å¼ï¼‰ -------")
        builder = FeatureBuilder(feat_cfg, log_fn=print)
        df = builder.fit_transform(df, target_col=args.target)
        print(f"ç‰¹å¾å·¥ç¨‹æ–°åˆ—æ•°ï¼š{len(builder.created_)}")

    # è®­ç»ƒ
    model_params = _parse_params(args.params)
    print("\n======= å¼€å§‹è®­ç»ƒ =======")
    pipe, report = tabular_pipeline.run_df(
        df, target=args.target, model=args.model,
        test_size=args.test_size, random_state=args.random_state,
        preprocess=(not args.no_preprocess), **model_params
    )
    print("======= è®­ç»ƒå®Œæˆï¼Œè¯„ä¼°æŠ¥å‘Š =======")
    print(json.dumps(report, ensure_ascii=False, indent=2))

    # å¯¼å‡º
    if args.export:
        out_dir = Path(args.out_dir) if args.out_dir else Path("runs") / datetime.now().strftime("%Y%m%d-%H%M%S")
        (out_dir / "features_config.json").write_text(builder.to_json(), encoding="utf-8")
        out_dir.mkdir(parents=True, exist_ok=True)
        joblib.dump(pipe, out_dir / "model.joblib")
        (out_dir / "report.json").write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"ğŸ“¦ å·²ä¿å­˜: {out_dir/'model.joblib'} / {out_dir/'report.json'}", file=sys.stderr)

if __name__ == "__main__":
    main()
