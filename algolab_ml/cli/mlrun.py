#!/usr/bin/env python
from __future__ import annotations
import argparse, json, sys, subprocess
from pathlib import Path
from datetime import datetime
from copy import deepcopy
import numpy as np
import pandas as pd

# --- JSON åºåˆ—åŒ–å…œåº•ï¼šnumpy/pandas â†’ çº¯ Python ---
def _json_default(o):
    import numpy as _np
    import pandas as _pd
    if isinstance(o, _np.generic):
        return o.item()
    if isinstance(o, _np.ndarray):
        return o.tolist()
    if isinstance(o, _pd.Timestamp):
        return o.isoformat()
    if hasattr(_pd, "NA") and o is _pd.NA:
        return None
    if isinstance(o, _pd.Series):
        return o.tolist()
    if isinstance(o, _pd.DataFrame):
        return {"columns": list(o.columns), "data": o.to_dict(orient="records")}
    return str(o)

def _save_json(path, obj):
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2, default=_json_default)

# â€”â€” å¯é€‰å·¥å…·ï¼šè‹¥åŒ…å†… utils ä¸å­˜åœ¨ï¼Œåˆ™ç”¨æœ¬æ–‡ä»¶å†…å…œåº•å®ç°
try:
    from algolab_ml.utils.artifacts import (
        dump_json, dump_joblib, versions_summary, git_commit_sha, make_run_dir
    )
except Exception:
    import joblib, platform, importlib
    def dump_json(p: Path, obj):
        p.parent.mkdir(parents=True, exist_ok=True)
        p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
    def dump_joblib(p: Path, obj):
        p.parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(obj, p)
    def versions_summary():
        pkgs = ["python", "pandas", "numpy", "scikit-learn", "lightgbm", "xgboost"]
        out = {"platform": platform.platform()}
        for name in pkgs:
            try:
                if name == "python":
                    out["python"] = sys.version
                else:
                    m = importlib.import_module(name.replace("-", "_"))
                    out[name] = getattr(m, "__version__", "unknown")
            except Exception:
                out[name] = "missing"
        return out
    def git_commit_sha():
        try:
            return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
        except Exception:
            return None
    def make_run_dir(root: Path) -> Path:
        ts = datetime.now().strftime("%Y%m%d-%H%M%S")
        d = root / ts
        d.mkdir(parents=True, exist_ok=True)
        return d

from algolab_ml.pipelines import tabular as tabular_pipeline
from algolab_ml.models import model_zoo
from algolab_ml.data.cleaning import (
    basic_clean, fill_na, enforce_schema, drop_constant_cols,
    clip_outliers, bucket_rare_categories, parse_dates
)
from algolab_ml.utils.config import load_json_or_yaml
from algolab_ml.features.builder import FeatureBuilder

# å¯é€‰ï¼šç»˜å›¾ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™è‡ªåŠ¨è·³è¿‡ç»˜å›¾ï¼‰
try:
    from algolab_ml.utils.plots import (
        plot_roc_pr_curves,
        plot_feature_importance,
        save_cv_results,
        plot_learning_curve,
        plot_confusion_matrix,
    )
except Exception:
    def plot_roc_pr_curves(*args, **kwargs): pass
    def plot_feature_importance(*args, **kwargs): pass
    def save_cv_results(*args, **kwargs): pass
    def plot_learning_curve(*args, **kwargs): pass
    def plot_confusion_matrix(*args, **kwargs): pass


# ----------------- æ—¥å¿—å·¥å…· -----------------
def _sec(title: str): print(f"\n------- {title} -------")
def _print_df_shape(df: pd.DataFrame, note: str): print(f"{note}ï¼š{df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")


def _parse_params(s: str | None) -> dict:
    if not s: return {}
    s = s.strip()
    if s.startswith("@"):
        p = Path(s[1:])
        data = json.loads(p.read_text(encoding="utf-8"))
        if not isinstance(data, dict): raise ValueError("params æ–‡ä»¶å¿…é¡»æ˜¯ JSON å¯¹è±¡")
        return data
    try:
        data = json.loads(s)
        if isinstance(data, dict): return data
    except Exception:
        pass
    out = {}
    for part in s.split(","):
        part = part.strip()
        if not part: continue
        if "=" not in part: raise ValueError(f"Bad param '{part}', expected key=value")
        k, v = part.split("=", 1); k, v = k.strip(), v.strip()
        if v.lower() in ("true","false"): out[k] = (v.lower()=="true"); continue
        try: out[k] = int(v); continue
        except: pass
        try: out[k] = float(v); continue
        except: pass
        out[k] = v
    return out


def _print_list_models(fmt: str, task: str):
    data = {
        "classification": {
            "canonical": model_zoo.available_models("classification"),
            "aliases": model_zoo.aliases_for_task("classification"),
        },
        "regression": {
            "canonical": model_zoo.available_models("regression"),
            "aliases": model_zoo.aliases_for_task("regression"),
        },
    }
    if fmt == "json":
        print(json.dumps(data, ensure_ascii=False, indent=2))
    else:
        scopes = ["classification","regression"] if task=="all" else [task]
        for sc in scopes:
            print(f"[{sc}]")
            print("  models: " + ", ".join(sorted(data[sc]["canonical"])))
            print("  aliases:")
            for k, v in sorted(data[sc]["aliases"].items()):
                print(f"    {k} -> {v}")


def _print_params(name: str):
    out = {}
    for task in ("classification","regression"):
        try:
            out[task] = model_zoo.model_signature(name, task)
        except KeyError:
            pass
    if not out:
        avail = {
            "classification": model_zoo.available_models("classification"),
            "regression": model_zoo.available_models("regression"),
        }
        raise SystemExit(f"Unknown model alias '{name}'. Available: {json.dumps(avail, ensure_ascii=False)}")
    print(json.dumps(out, ensure_ascii=False, indent=2))


# ----------------- å¸¦æ—¥å¿—çš„æ¸…æ´—æ‰§è¡Œ -----------------
def _apply_cleaning_with_log(df: pd.DataFrame, cfg: dict, target: str | None = None) -> pd.DataFrame:
    cur = df.copy()
    _sec("å¼€å§‹æ•°æ®æ¸…æ´—ï¼ˆé…ç½®å¼ï¼‰")
    _print_df_shape(cur, "åŸå§‹æ•°æ®ç»´åº¦")

    # 1) enforce_schema
    es = cfg.get("enforce_schema")
    if es:
        _sec("Schema è§„èŒƒåŒ–")
        rename = es.get("rename") or {}
        required = es.get("required") or []
        dtypes = es.get("dtypes") or {}
        if rename:  print(f"é‡å‘½ååˆ—ï¼š{rename}")
        if required: print(f"å¿…éœ€åˆ—ï¼š{required}")
        if dtypes:  print(f"ç±»å‹çº¦æŸï¼ˆç›®æ ‡ç±»å‹ï¼‰ï¼š{dtypes}")
        cur = enforce_schema(cur, dtypes=dtypes, rename=rename, required=required)
        _print_df_shape(cur, "è§„èŒƒåŒ–åç»´åº¦")

    # 2) basic_clean
    bc = cfg.get("basic_clean")
    if bc:
        drop_cols = bc.get("drop_cols") if isinstance(bc, dict) else None
        dup_cnt = cur.duplicated().sum()
        before_cols = set(cur.columns)
        cur2 = basic_clean(cur, drop_cols=drop_cols)
        dropped_cols = list(before_cols - set(cur2.columns))
        print(f"å»é‡è¡Œæ•°ï¼š{dup_cnt}")
        if drop_cols:
            kept = [c for c in drop_cols if c in before_cols]
            print(f"åˆ é™¤æŒ‡å®šåˆ—ï¼š{kept or []}")
        if dropped_cols:
            print(f"ï¼ˆå«é‡å¤å/æ— æ•ˆå¯¼è‡´çš„ï¼‰å®é™…è¢«åˆ é™¤åˆ—ï¼š{dropped_cols}")
        cur = cur2
        _print_df_shape(cur, "basic_clean åç»´åº¦")

    # 3) fill_na
    if cfg.get("fill_na"):
        na = cfg["fill_na"]
        num = na.get("num","median")
        cat = na.get("cat","most_frequent")
        num_cols = cur.select_dtypes(include=["number"]).columns
        cat_cols = [c for c in cur.columns if c not in num_cols]
        n_na_num_before = int(cur[num_cols].isna().sum().sum()) if len(num_cols)>0 else 0
        n_na_cat_before = int(cur[cat_cols].isna().sum().sum()) if len(cat_cols)>0 else 0
        cur2 = fill_na(cur, num_strategy=num, cat_strategy=cat)
        n_na_num_after = int(cur2[num_cols].isna().sum().sum()) if len(num_cols)>0 else 0
        n_na_cat_after = int(cur2[cat_cols].isna().sum().sum()) if len(cat_cols)>0 else 0
        _sec("ç¼ºå¤±å€¼å¡«å……")
        print(f"æ•°å€¼åˆ—å¡«å……ç­–ç•¥ï¼š{num}ï¼Œç¼ºå¤± {n_na_num_before} -> {n_na_cat_after if False else n_na_num_after}")
        print(f"ç±»åˆ«åˆ—å¡«å……ç­–ç•¥ï¼š{cat}ï¼Œç¼ºå¤± {n_na_cat_before} -> {n_na_cat_after}")
        cur = cur2

    # 4) drop_constant
    if cfg.get("drop_constant"):
        th = cfg["drop_constant"].get("threshold_unique", 1)
        const_cols = [c for c in cur.columns if cur[c].nunique(dropna=True) <= th]
        if target and target in const_cols:
            const_cols = [c for c in const_cols if c != target]
            print(f"å·²è‡ªåŠ¨æ’é™¤ç›®æ ‡åˆ—ï¼š{target}")
        cur = drop_constant_cols(cur, threshold_unique=th)
        _sec("åˆ é™¤å¸¸é‡åˆ—")
        print(f"é˜ˆå€¼ï¼šå”¯ä¸€å€¼ â‰¤ {th}ï¼Œåˆ é™¤åˆ—ï¼š{const_cols}")

    # 5) clip_outliers
    if cfg.get("clip_outliers"):
        co = cfg["clip_outliers"]
        method = co.get("method","iqr")
        zt = co.get("z_thresh",3.0)
        ik = co.get("iqr_k",1.5)
        cols = co.get("cols")
        if cols is None:
            cols = cur.select_dtypes(include=["number"]).columns.tolist()
        if target and target in cols:
            cols = [c for c in cols if c != target]
            print(f"å·²è‡ªåŠ¨æ’é™¤ç›®æ ‡åˆ—ï¼š{target}")
        if not cols:
            _sec("å¼‚å¸¸å€¼æˆªæ–­"); print("æ²¡æœ‰å¯å¤„ç†çš„æ•°å€¼åˆ—ï¼Œè·³è¿‡ã€‚")
        else:
            before = cur[cols].copy()
            cur = clip_outliers(cur, cols=cols, method=method, z_thresh=zt, iqr_k=ik)
            changed = {c: int((cur[c] != before[c]).sum()) for c in cols if c in cur.columns}
            total_changed = int(sum(changed.values()))
            _sec("å¼‚å¸¸å€¼æˆªæ–­")
            print(f"æ–¹å¼ï¼š{method}ï¼ˆz={zt} / iqr_k={ik}ï¼‰ï¼Œå¤„ç†åˆ—æ•°ï¼š{len(cols)}ï¼Œè¢«æˆªæ–­çš„å•å…ƒæ ¼æ€»æ•°ï¼š{total_changed}")
            if total_changed:
                top = sorted(changed.items(), key=lambda x: x[1], reverse=True)[:8]
                print(f"å˜åŒ–æœ€å¤šçš„åˆ—ï¼ˆå‰ 8ï¼‰ï¼š{top}")

    # 6) bucket_rare
    if cfg.get("bucket_rare"):
        br = cfg["bucket_rare"]
        cols = br.get("cols", [])
        minf = br.get("min_freq", 10)
        other = br.get("other_label","_OTHER")
        counts = {}
        for c in cols:
            if c in cur.columns:
                vc = cur[c].astype(str).value_counts(dropna=False)
                counts[c] = int((vc < minf).sum())
        cur = bucket_rare_categories(cur, cols=cols, min_freq=minf, other_label=other)
        _sec("ä½é¢‘ç±»åˆ«åˆå¹¶")
        print(f"åˆ—ï¼š{cols}ï¼Œé¢‘æ¬¡é˜ˆå€¼ï¼š{minf}ï¼Œåˆå¹¶æ ‡ç­¾ï¼š{other}")
        if counts:
            print(f"å„åˆ—ä½é¢‘ç±»åˆ«ä¸ªæ•°ï¼š{counts}")

    # 7) parse_dates
    if cfg.get("parse_dates"):
        mapping = cfg["parse_dates"]
        cur = parse_dates(cur, mapping)
        _sec("æ—¥æœŸè§£æ")
        info = {}
        for col in mapping:
            if col in cur.columns:
                ok = int(cur[col].notna().sum())
                na = int(cur[col].isna().sum())
                info[col] = {"å¯è§£æ": ok, "NaT": na}
        print(f"è§£æåˆ—ï¼š{mapping}ï¼Œç»“æœï¼š{info}")

    _sec("æ¸…æ´—å®Œæˆ")
    _print_df_shape(cur, "æœ€ç»ˆæ•°æ®ç»´åº¦")
    return cur


# ----------------- é¢„æµ‹/æ‰“åˆ†ï¼ˆè®­ç»ƒåæˆ–çº¯é¢„æµ‹æ¨¡å¼é€šç”¨ï¼‰ -----------------
def _run_predict(
    pipe,
    df_pred: pd.DataFrame,
    *,
    proba: bool = False,
    id_cols: list[str] | None = None,
    out_path: str | None = None,
    task_hint: str | None = None,
    expected_input_cols: list[str] | None = None,
    target_col: str | None = None,   # è®­ç»ƒæ—¶çš„ target å
    apply_threshold: str | None = None,     # "from-metrics" æˆ– å…·ä½“æ•°å­—çš„å­—ç¬¦ä¸²
    threshold_value: float | None = None,   # æŒ‡å®šé˜ˆå€¼ï¼ˆä¼˜å…ˆçº§ä½äº apply_threshold=æ•°å­—ï¼‰
):
    # 1) åˆ†ç¦» id åˆ—
    id_cols = [c for c in (id_cols or []) if c in df_pred.columns]
    id_frame = df_pred[id_cols].copy() if id_cols else None

    # 2) å»æ‰ target åˆ—ï¼ˆé˜²æ­¢æ··å…¥ï¼‰
    if target_col and target_col in df_pred.columns:
        df_pred = df_pred.drop(columns=[target_col])

    # 3) è®­ç»ƒæœŸçš„è¾“å…¥åˆ—ï¼ˆå¹¶ç¡®ä¿ä¸å« targetï¼‰
    if expected_input_cols is None:
        expected_input_cols = getattr(pipe, "feature_names_in_", None)
        if expected_input_cols is None:
            expected_input_cols = [c for c in df_pred.columns if c not in id_cols]
    if target_col:
        expected_input_cols = [c for c in expected_input_cols if c != target_col]

    # 4) å¯¹é½åˆ—
    drop_cols = set(id_cols)
    X_pred = df_pred.drop(columns=list(drop_cols & set(df_pred.columns)), errors="ignore")
    X_pred = X_pred.reindex(columns=list(expected_input_cols), fill_value=np.nan)

    # 5) é¢„æµ‹
    need_proba_for_thr = bool(apply_threshold and str(apply_threshold).strip() != "")
    do_proba = bool(proba or need_proba_for_thr)

    out_df = None
    prob = None
    if do_proba and hasattr(pipe, "predict_proba"):
        prob = pipe.predict_proba(X_pred)
        proba_cols = [f"proba_{i}" for i in range(prob.shape[1])]
        if proba:
            out_df = pd.DataFrame(prob, columns=proba_cols)

    if apply_threshold and str(apply_threshold).strip() != "":
        # ä»…æ”¯æŒäºŒåˆ†ç±»é˜ˆå€¼
        if prob is None:
            # æ²¡æœ‰ prob èƒ½åŠ›
            print("âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒ predict_probaï¼Œæ— æ³•åº”ç”¨é˜ˆå€¼ã€‚æ”¹ç”¨ç›´æ¥é¢„æµ‹ã€‚")
            pred = pipe.predict(X_pred)
            out_df = pd.DataFrame({"pred": pred})
        else:
            # è§£æé˜ˆå€¼
            thr = None
            s = str(apply_threshold).strip().lower()
            if s == "from-metrics" and threshold_value is not None:
                thr = float(threshold_value)
            else:
                # s å¯èƒ½å°±æ˜¯æ•°å­—
                try:
                    thr = float(s)
                except Exception:
                    if threshold_value is not None:
                        thr = float(threshold_value)
            if thr is None:
                thr = 0.5
            # å–æ­£ç±»æ¦‚ç‡ï¼ˆçº¦å®šç¬¬ 2 åˆ—ï¼‰
            if prob.ndim == 2 and prob.shape[1] >= 2:
                pos = prob[:, 1]
            else:
                pos = prob.ravel()
            pred_bin = (pos >= float(thr)).astype(int)
            if out_df is None:
                out_df = pd.DataFrame({"pred": pred_bin})
            else:
                out_df = out_df.copy()
                out_df.insert(0, "pred", pred_bin)
    else:
        # æ— é˜ˆå€¼ï¼šæŒ‰ç”¨æˆ·éœ€æ±‚å†³å®šè¾“å‡ºæ˜¯ä»€ä¹ˆ
        if out_df is None:
            # æ—¢æ²¡è¦æ¦‚ç‡ä¹Ÿæ²¡è¦é˜ˆå€¼ï¼šè¾“å‡ºæ ‡ç­¾
            pred = pipe.predict(X_pred)
            out_df = pd.DataFrame({"pred": pred})

    # 6) æ‹¼å› id åˆ— & å¯¼å‡º
    if id_frame is not None:
        out_df = pd.concat([id_frame.reset_index(drop=True), out_df.reset_index(drop=True)], axis=1)

    if out_path:
        Path(out_path).parent.mkdir(parents=True, exist_ok=True)
        out_df.to_csv(out_path, index=False)
        print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°ï¼š{Path(out_path).resolve()}")
    else:
        print("ï¼ˆæœªæä¾› --predict-outï¼Œä»¥ä¸‹ä¸ºé¢„æµ‹ç»“æœé¢„è§ˆï¼‰")
        print(out_df.head(10).to_string(index=False))


def main():
    ap = argparse.ArgumentParser(description="Algolab ML Runnerï¼ˆè‡ªåŠ¨/æŒ‡å®šä»»åŠ¡ï¼ŒCV/æ—©åœ/å¯è§†åŒ–ï¼Œå¸¦æ¸…æ´—æ—¥å¿— + æ¨ç†ï¼‰")
    # è®­ç»ƒç›¸å…³
    ap.add_argument("--csv", help="è®­ç»ƒæ•°æ® CSV è·¯å¾„")
    ap.add_argument("--target", help="ç›®æ ‡åˆ—å")
    ap.add_argument("--model", default="xgb", help="xgb|lgbm|catboost|rf|gbdt|logreg|ridge|lassoï¼ˆæ”¯æŒåˆ«åï¼‰")
    ap.add_argument("--test_size", type=float, default=0.2)
    ap.add_argument("--random_state", type=int, default=42)
    ap.add_argument("--no-preprocess", dest="no_preprocess", action="store_true", help="å…³é—­æ ‡å‡†åŒ–/One-Hot ç­‰é¢„å¤„ç†")
    ap.add_argument("--task", choices=["classification","regression","auto"], default="auto", help="è¦†ç›–è‡ªåŠ¨è¯†åˆ«ï¼›é»˜è®¤ auto")
    # æ¸…æ´— & ç‰¹å¾å·¥ç¨‹
    ap.add_argument("--clean-config", default=None, help="æ¸…æ´—é…ç½®ï¼šJSON æ–‡æœ¬æˆ– @/path/to/clean.yml(JSON/YAML)")
    ap.add_argument("--feat-config",  default=None, help="ç‰¹å¾é…ç½®ï¼šJSON æ–‡æœ¬æˆ– @/path/to/features.yml(JSON/YAML)")
    # CV / æœç´¢
    ap.add_argument("--cv", type=int, default=0, help="äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆ0/1 è¡¨ç¤ºä¸åš CVï¼‰")
    ap.add_argument("--search", choices=["grid","random"], default="grid", help="è¶…å‚æœç´¢æ–¹å¼")
    ap.add_argument("--n-iter", type=int, default=20, help="RandomizedSearch çš„è¿­ä»£æ¬¡æ•°")
    ap.add_argument("--param-grid", default=None, help="æŒ‡å®šæœç´¢ç©ºé—´ï¼ˆJSON æˆ– @/path/file.jsonï¼‰ï¼Œä¸å¡«ç”¨å†…ç½®é»˜è®¤")
    ap.add_argument("--scoring", default=None, help="scoring åç§°ï¼ˆå¦‚ roc_auc / accuracy / f1_macro / r2 / neg_root_mean_squared_error ç­‰ï¼‰")
    # æ—©åœ
    ap.add_argument("--early-stopping", action="store_true", help="å¯ç”¨æ—©åœï¼ˆLightGBM/XGBoost ç”Ÿæ•ˆï¼‰")
    ap.add_argument("--val-size", type=float, default=0.15, help="æ—©åœçš„éªŒè¯é›†å è®­ç»ƒé›†æ¯”ä¾‹")
    ap.add_argument("--es-rounds", type=int, default=50, help="æ²¡æœ‰æå‡çš„å®¹å¿è¿­ä»£æ•°")
    ap.add_argument("--eval-metric", default=None, help="eval_metricï¼ˆå¦‚ auc / logloss / rmse ç­‰ï¼‰")
    # ç¬¬ 6 æ­¥ï¼šæ ·æœ¬/ä¸å¹³è¡¡
    ap.add_argument("--sample-weight-col", default=None, help="æ ·æœ¬æƒé‡åˆ—åï¼ˆå¯é€‰ï¼‰")
    ap.add_argument("--class-weight", default=None, choices=[None, "balanced", "balanced_subsample", "balanced"], help="åˆ†ç±»ï¼šclass_weightã€‚å¸¸ç”¨ 'balanced'")
    # ç¬¬ 7 æ­¥ï¼šé˜ˆå€¼è°ƒä¼˜
    ap.add_argument("--optimize-metric", default=None, help="äºŒåˆ†ç±»é˜ˆå€¼è°ƒä¼˜æŒ‡æ ‡ï¼ˆå¦‚ f1 / f1_macro / recall / precision / accuracy / youden_jï¼‰")
    ap.add_argument("--threshold-grid", default="auto", help="é˜ˆå€¼ç½‘æ ¼ï¼š'auto' æˆ– '0.1:0.9:0.01' æˆ–é€—å·åˆ—è¡¨")
    # å¯¼å‡º
    ap.add_argument("--export", action="store_true", help="ä¿å­˜æ¨¡å‹ä¸æŠ¥å‘Šåˆ° --out-dirï¼ˆæœªæŒ‡å®šåˆ™ runs/æ—¶é—´æˆ³ï¼‰")
    ap.add_argument("--out-dir", default=None, help="ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ runs/YYYYmmdd-HHMMSSï¼‰")
    ap.add_argument("--params", default=None, help="æ¨¡å‹è¶…å‚æ•°ï¼ŒJSON æˆ– 'k=v,k2=v2' æˆ– @/path/params.json")
    # é¢„æµ‹/æ‰“åˆ†
    ap.add_argument("--model-path", default=None, help="å·²æœ‰æ¨¡å‹ç®¡é“è·¯å¾„ï¼ˆpipeline.joblibï¼‰ï¼Œä»…é¢„æµ‹æ¨¡å¼å¿…å¡«æˆ–é…åˆ --run-dir")
    ap.add_argument("--run-dir", default=None, help="è®­ç»ƒå¯¼å‡ºç›®å½•ï¼ˆåŒ…å« pipeline.joblib / columns.json / features_config.json / clean_config.json / metrics.jsonï¼‰")
    ap.add_argument("--predict-csv", default=None, help="éœ€è¦æ‰“åˆ†çš„ CSVï¼ˆå¯é…åˆ --model-path æˆ–è®­ç»ƒåç›´æ¥æ‰“åˆ†ï¼‰")
    ap.add_argument("--predict-out", default=None, help="é¢„æµ‹ç»“æœè¾“å‡º CSVï¼ˆé»˜è®¤æ‰“å°å‰ 10 è¡Œï¼‰")
    ap.add_argument("--proba", action="store_true", help="åˆ†ç±»ä»»åŠ¡è¾“å‡ºæ¦‚ç‡åˆ— proba_*")
    ap.add_argument("--id-cols", default=None, help="é€—å·åˆ†éš”çš„ id åˆ—åï¼ŒåŸæ ·æ‹·è´åˆ°é¢„æµ‹ç»“æœ")
    # é¢„æµ‹æ—¶åº”ç”¨é˜ˆå€¼
    ap.add_argument("--apply-threshold", default=None, help="é¢„æµ‹æ—¶åº”ç”¨é˜ˆå€¼ï¼š'from-metrics' æˆ–å…·ä½“æ•°å­—ï¼ˆå¦‚ 0.37ï¼‰")
    ap.add_argument("--threshold-value", type=float, default=None, help="ä¸ --apply-threshold é…åˆæ˜¾å¼ç»™å€¼ï¼›è‹¥ --apply-threshold=from-metrics åˆ™ä» metrics.json è¯»å–")

    # å‘ç°æ€§
    ap.add_argument("--list-models", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹ä¸åˆ«å")
    ap.add_argument("--format", choices=["json","text"], default="json", help="list è¾“å‡ºæ ¼å¼")
    ap.add_argument("--task-scope", choices=["classification","regression","all"], default="all", help="list ä½œç”¨åŸŸ")
    ap.add_argument("--show-params", default=None, help="æ˜¾ç¤ºæŸæ¨¡å‹æ„é€ å‡½æ•°å‚æ•°ç­¾ååé€€å‡º")
    args, _ = ap.parse_known_args()

    # å‘ç°ç±»å‘½ä»¤
    if args.list_models:
        _print_list_models(args.format, args.task_scope); return
    if args.show_params:
        _print_params(args.show_params); return

    # ----------------- çº¯â€œé¢„æµ‹æ¨¡å¼â€ï¼ˆä¸è®­ç»ƒï¼‰ -----------------
    if args.predict_csv and not args.csv:
        if not args.model_path and not args.run_dir:
            raise SystemExit("ä»…é¢„æµ‹æ¨¡å¼éœ€è¦æä¾› --model-path æˆ– --run-dir")

        # è§£æè·¯å¾„
        run_dir = Path(args.run_dir) if args.run_dir else None
        model_path = Path(args.model_path) if args.model_path else None
        if run_dir and not model_path:
            mp1 = run_dir / "pipeline.joblib"
            if mp1.exists():
                model_path = mp1
        if not model_path or not model_path.exists():
            raise SystemExit(f"æ‰¾ä¸åˆ°æ¨¡å‹ï¼š{model_path or '(æœªæä¾›)'}")

        # è¯»å–å¾…é¢„æµ‹
        df_pred = pd.read_csv(args.predict_csv)

        # è®­ç»ƒæ—¶å…ƒæ•°æ®
        expected_cols = None
        target_col = getattr(args, "target", None)
        tuned_thr = None
        if run_dir:
            cols_file = run_dir / "columns.json"
            if cols_file.exists():
                cols_json = json.loads(cols_file.read_text(encoding="utf-8"))
                all_cols = cols_json.get("all_columns") or []
                tgt = cols_json.get("target")
                if all_cols:
                    expected_cols = [c for c in all_cols if c != tgt]
                if tgt:
                    target_col = tgt
            met = run_dir / "metrics.json"
            if met.exists():
                met_json = json.loads(met.read_text(encoding="utf-8"))
                tuned_thr = (met_json.get("threshold_tuning") or {}).get("best_threshold")

        # æ¸…æ´—/ç‰¹å¾å·¥ç¨‹ï¼ˆtransformï¼‰
        clean_cfg = None
        feat_cfg = None
        builder = None
        if run_dir:
            cc = run_dir / "clean_config.json"
            fc = run_dir / "features_config.json"
            if cc.exists():
                clean_cfg = json.loads(cc.read_text(encoding="utf-8"))
                if isinstance(clean_cfg, dict) and "drop_constant" in clean_cfg:
                    print("âš ï¸  é¢„æµ‹é˜¶æ®µä¸ºä¿æŒåˆ—ä¸€è‡´ï¼Œå·²è·³è¿‡ drop_constantï¼ˆåˆ é™¤å¸¸é‡åˆ—ï¼‰")
                    clean_cfg = deepcopy(clean_cfg)
                    clean_cfg.pop("drop_constant", None)
                df_pred = _apply_cleaning_with_log(df_pred, clean_cfg, target=target_col)
            if fc.exists():
                feat_cfg = json.loads(fc.read_text(encoding="utf-8"))
                if isinstance(feat_cfg, dict) and feat_cfg:
                    print("\n------- é¢„æµ‹é˜¶æ®µï¼šæŒ‰è®­ç»ƒæ—¶é…ç½®åšç‰¹å¾å·¥ç¨‹ï¼ˆtransformï¼‰ -------")
                    builder = FeatureBuilder(feat_cfg, log_fn=print)
                    df_pred = builder.transform(df_pred)

        # åŠ è½½ pipeline å¹¶é¢„æµ‹
        from joblib import load as _load
        pipe = _load(model_path)
        id_cols = [s.strip() for s in args.id_cols.split(",")] if args.id_cols else None
        out_path = Path(args.predict_out) if args.predict_out else None

        _run_predict(
            pipe,
            df_pred,
            proba=bool(getattr(args, "proba", False) or (args.apply_threshold not in (None, "", "0"))),
            id_cols=id_cols,
            out_path=str(out_path) if out_path else None,
            expected_input_cols=expected_cols,
            target_col=target_col,
            apply_threshold=args.apply_threshold,
            threshold_value=(args.threshold_value if args.apply_threshold != "from-metrics" else tuned_thr),
        )
        return

    # ----------------- è®­ç»ƒè·¯å¾„ -----------------
    if not args.csv or not args.target:
        raise SystemExit("è®­ç»ƒæ¨¡å¼éœ€è¦ --csv / --targetã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨â€œä»…é¢„æµ‹æ¨¡å¼â€ã€‚")

    # è¯»å– & æ¸…æ´—
    df = pd.read_csv(args.csv)
    clean_cfg = load_json_or_yaml(args.clean_config)
    if clean_cfg:
        df = _apply_cleaning_with_log(df, clean_cfg, target=args.target)

    # ç‰¹å¾å·¥ç¨‹
    feat_cfg = load_json_or_yaml(args.feat_config)
    builder = None
    if isinstance(feat_cfg, dict) and feat_cfg:
        print("\n------- å¼€å§‹ç‰¹å¾å·¥ç¨‹ï¼ˆé…ç½®å¼ï¼‰ -------")
        builder = FeatureBuilder(feat_cfg, log_fn=print)
        df = builder.fit_transform(df, target_col=args.target)
        print(f"ç‰¹å¾å·¥ç¨‹æ–°åˆ—æ•°ï¼š{len(builder.created_)}")
    else:
        print("\n------- ç‰¹å¾å·¥ç¨‹å®Œæˆ -------")
        print("æœ¬æ¬¡æ–°å¢ç‰¹å¾åˆ—æ€»æ•°ï¼š0")
        print("ç‰¹å¾å·¥ç¨‹æ–°åˆ—æ•°ï¼š0")

    # æ ·æœ¬æƒé‡
    sw = None
    if getattr(args, "sample_weight_col", None):
        col = str(args.sample_weight_col)
        if col in df.columns:
            sw = df[col].values
        else:
            print(f"âš ï¸ sample_weight_col='{col}' ä¸åœ¨æ•°æ®åˆ—ä¸­ï¼Œå·²å¿½ç•¥ã€‚")

    # è®­ç»ƒ
    model_params = _parse_params(getattr(args, "params", None))
    print("\n======= å¼€å§‹è®­ç»ƒ =======")
    pipe, report = tabular_pipeline.run_df(
        df, target=args.target, model=args.model,
        test_size=args.test_size, random_state=args.random_state,
        preprocess=(not args.no_preprocess),
        model_params=model_params,
        task=(None if args.task == "auto" else args.task),
        cv=args.cv, search=args.search, n_iter=args.n_iter,
        scoring=args.scoring, param_grid=args.param_grid,
        early_stopping=args.early_stopping, val_size=args.val_size,
        es_rounds=args.es_rounds, eval_metric=args.eval_metric,
        sample_weight=sw,
        class_weight=(args.class_weight if isinstance(args.class_weight, str) else None),
        optimize_metric=args.optimize_metric,
        threshold_grid=args.threshold_grid,
    )
    print("======= è®­ç»ƒå®Œæˆï¼Œè¯„ä¼°æŠ¥å‘Š =======")
    print(json.dumps(report, ensure_ascii=False, indent=2, default=_json_default))

    # å¯¼å‡º
    if args.export:
        out_dir = Path(args.out_dir) if args.out_dir else make_run_dir(Path("runs"))
        out_dir.mkdir(parents=True, exist_ok=True)
        dump_joblib(out_dir / "pipeline.joblib", pipe)
        report_to_save = {k: v for k, v in report.items() if not str(k).startswith("_")}
        _save_json(out_dir / "metrics.json", report_to_save)
        _save_json(out_dir / "columns.json", {"target": args.target, "all_columns": df.columns.tolist()})
        _save_json(out_dir / "versions.json", versions_summary())
        if clean_cfg: _save_json(out_dir / "clean_config.json", clean_cfg)
        if builder is not None:
            try:
                _save_json(out_dir / "features_config.json", builder.to_dict())
            except Exception:
                if feat_cfg: _save_json(out_dir / "features_config.json", feat_cfg)

        # æ›²çº¿ä¸å¯è§†åŒ–
        try:
            if report.get("task") == "classification" and "roc_auc" in report and "_y_true" in report and "_y_prob" in report:
                plot_roc_pr_curves(report["_y_true"], report["_y_prob"], out_dir)
        except Exception:
            pass
        try:
            feature_names = getattr(pipe, "feature_names_in_", None)
            plot_feature_importance(pipe, feature_names, out_dir, top_n=30)
        except Exception:
            pass
        try:
            save_cv_results(pipe, out_dir)
        except Exception:
            pass
        try:
            ev = report.get("evals_result")
            if ev:
                plot_learning_curve(ev, out_dir, metric_hint=args.eval_metric, task=report.get("task"))
        except Exception:
            pass
        try:
            if report.get("task") == "classification":
                y_true = report.get("_y_true")
                y_pred = report.get("_y_pred")
                if y_true is not None and y_pred is not None:
                    plot_confusion_matrix(y_true, y_pred, out_dir)
        except Exception:
            pass

        print(f"ğŸ“¦ è®­ç»ƒå·¥ä»¶å·²å¯¼å‡ºåˆ°: {out_dir.resolve()}")

    # ------- è®­ç»ƒåç«‹å³å¯¹æ–° CSV é¢„æµ‹ -------
    pred_csv = getattr(args, "predict_csv", None)
    pred_out = getattr(args, "predict_out", None)
    proba    = bool(getattr(args, "proba", False) or (args.apply_threshold not in (None, "", "0")))
    id_cols_arg = getattr(args, "id_cols", None)

    if pred_csv:
        p = Path(pred_csv)
        if not p.exists():
            print(f"âš ï¸  é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{pred_csv}ï¼Œå·²è·³è¿‡é¢„æµ‹ã€‚")
        else:
            print("\n------- è®­ç»ƒåç«‹å³å¯¹æ–° CSV é¢„æµ‹ -------")
            df_pred_raw = pd.read_csv(p)

            # 1) æ¸…æ´—
            if clean_cfg:
                clean_cfg_pred = deepcopy(clean_cfg)
                if isinstance(clean_cfg_pred, dict) and "drop_constant" in clean_cfg_pred:
                    print("âš ï¸  é¢„æµ‹é˜¶æ®µä¸ºä¿æŒåˆ—ä¸€è‡´ï¼Œå·²è·³è¿‡ drop_constantï¼ˆåˆ é™¤å¸¸é‡åˆ—ï¼‰")
                    clean_cfg_pred.pop("drop_constant", None)
                df_pred = _apply_cleaning_with_log(df_pred_raw, clean_cfg_pred, target=args.target)
            else:
                df_pred = df_pred_raw.copy()

            # 2) ç‰¹å¾å·¥ç¨‹
            if builder is not None:
                try:
                    df_pred = builder.transform(df_pred)
                except Exception as e:
                    print(f"âš ï¸ ç‰¹å¾å·¥ç¨‹åœ¨é¢„æµ‹é˜¶æ®µ transform å¤±è´¥ï¼š{e}ã€‚å°†ç›´æ¥ç”¨åŸå§‹åˆ—è¿›è¡Œé¢„æµ‹ã€‚")

            # 3) id åˆ—
            id_cols = []
            if id_cols_arg:
                id_cols = [c.strip() for c in str(id_cols_arg).split(",") if c.strip() and c.strip() in df_pred.columns]
            id_frame = df_pred[id_cols].copy() if id_cols else None

            # 4) å¯¹é½
            expected_cols = [c for c in df.columns if c != args.target]
            drop_cols = [args.target] if args.target in df_pred.columns else []
            drop_cols += [c for c in id_cols if c in df_pred.columns]
            X_pred = df_pred.drop(columns=drop_cols, errors="ignore")
            X_pred = X_pred.reindex(columns=expected_cols, fill_value=np.nan)

            # æ‰¾åˆ°è®­ç»ƒæ—¶çš„æœ€ä½³é˜ˆå€¼ï¼ˆå¦‚æœ‰ï¼‰
            tuned_thr = None
            tt = (report.get("threshold_tuning") or {})
            if "best_threshold" in tt:
                tuned_thr = tt["best_threshold"]

            # 5) é¢„æµ‹ï¼ˆç»Ÿä¸€èµ° _run_predictï¼Œä¿è¯é€»è¾‘ä¸€è‡´ï¼‰
            # ç»„è£…å› df_pred ä»¥å¤ç”¨åˆ—å¯¹é½é€»è¾‘
            df_aligned = X_pred
            if id_frame is not None:
                for c in id_frame.columns:
                    df_aligned[c] = id_frame[c].values

            _run_predict(
                pipe,
                df_aligned,
                proba=proba,
                id_cols=id_cols,
                out_path=pred_out,
                expected_input_cols=expected_cols,
                target_col=args.target,
                apply_threshold=args.apply_threshold,
                threshold_value=(args.threshold_value if args.apply_threshold != "from-metrics" else tuned_thr),
            )


if __name__ == "__main__":
    main()
